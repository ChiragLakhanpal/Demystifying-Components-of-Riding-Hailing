---
title: "Predicting Gratuity Amount for Taxi Drivers"
author: "Chirag Lakhanpal, Shikha Sharma, Abhishek Pradhan"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

<style type="text/css">
  body{
  font-size: 10pt;
  font-family: 'Calibri';

@media (prefers-color-scheme: dark) {
  img {
    opacity: .75;
    transition: opacity .5s ease-in-out;
  }
  img:hover {
    opacity: 1;
  }
  }
}
</style>

```{r init, include=FALSE}
#list.of.packages <- c("arrow","ezids","knitr","dplyr","lubridate","ggplot2","reshape2","caret",
#                      "fastDummies","dataPreparation","factoextra","AICcmodavg","modelr","glmnet","rpart","MLmetrics","plotmo","rpart","rpart.plot",
#                      "tree","randomForest")

#new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
#if(length(new.packages)) install.packages(new.packages)

#install.packages("tree")
#remotes::install_github("physicsland/ezids")
#remove.packages("tree")
#install.packages("randomForest")

# Installing necessary packages 
library(arrow)
library(ezids)
library(knitr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(reshape2)
library(caret)
library(fastDummies)
library(dataPreparation)
library(factoextra)
library(AICcmodavg)
library(modelr)
library(glmnet)
library(rpart)
library(MLmetrics)
library(plotmo) 
library(rpart)
library(rpart.plot)
library(tree)
library(randomForest)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
#options(scientific=T, digits = 3) 
options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
#options(scipen=999)
```



# 1 Exordium

##### One of the most famous pictures of New York is the wave of yellow taxi taxis flooding the streets. So, where better to research taxi cab data than New York City? This is exactly what we intended to do. From 2009 until the present, the NYC Taxi and Limousine Commission (TLC) has gathered massive amounts of data for every taxi travel in New York City. We set out to get our hands dirty and put the sophisticated analysis,we learnt over the semester to work.


##### **We wanted to see how parameters like pick-up location, distance, number of passengers, and drop-off location impact the tipping behavior of NYC taxi drivers.**


# 2 Data Preparation

## 2.1 Data Gathering

```{r Reading_Data, results='markup',echo=FALSE}

# Loading main data for New York yellow taxi trips
raw_data <- data.frame(read_parquet('../Data/yellow_tripdata_2022-06.parquet'))

# Loading main data for Zone information
zone_lookup <- data.frame(read.csv("../Data/Taxi_Zone_Lookup.csv"))

data_des <- data.frame(read.csv('../Data/Data_Definitions.csv'))

kable(str(raw_data),bso='bordered', title = 'Glimpse of Raw Data')

```

##### **Comments** :  At a first glance, there are total `r nrow(raw_data)*ncol(raw_data)` observation across `r nrow(raw_data)` and `r ncol(raw_data)` variables in which 7 are categorical and 12 are numerical variables. The data was procured from the NYC Open Source GIS website - https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page.

## 2.2 Data Descriptors

```{r, results='markup',echo=FALSE}
xkabledply((data_des),bso='bordered', title = 'Zones', pos = 'center')
```

##### **Comments** : There are total 19 variables but not all are used in our analysis, we will shortly remove the irreverent columns. Some major columns are vital for this analysis are Trip distance, Trip duration, Fare amount , Tip amount, Passenger count and Vendor ID.

```{r, results='markup',echo=FALSE}
xkabledply(head((zone_lookup)),bso='bordered', title = 'Zones')
```

##### **Comments:** For this analysis NYC has been divided 6 Borough and 261 distinct Zones.
  
## 2.3 Data Statistics

```{r Checking Data types, results='markup',echo=FALSE}

# Deep dive into data

str(zone_lookup)

```

## 2.4 Data Manipulation

```{r Excess columns Removal,echo=FALSE}


drop <- c("store_and_fwd_flag", "extra", "mta_tax","improvement_surcharge","congestion_surcharge","airport_fee","total_amount")

raw_data <- raw_data[,!(colnames(raw_data) %in% drop)]
```



```{r Renaming columns,echo=FALSE}
colnames(raw_data)[2] <- 'pickup_time'
colnames(raw_data)[3] <- 'dropoff_time'
colnames(raw_data)[7] <- 'PULocation'
colnames(raw_data)[8] <- 'DOLocation'
```

```{r Summary for Raw Data,echo=FALSE}
#xkablesummary(raw_data,bso = 'bordered', title = 'Raw Data Summary', pos = 'center')
```

### 2.4.1 Look up values

```{r PU and DO Loop up,echo=FALSE}

# Looking up Zone for Zone IDs 

raw_data <- inner_join(raw_data,zone_lookup,by=c('PULocation' = 'LocationID'))

# Dropping irrelevant column added from above join

drop <- c('Zone','service_zone','PULocation')


raw_data <- raw_data[,!(colnames(raw_data) %in% drop)]

colnames(raw_data)[12] <- 'PULocation'

# Performing above exercise for drop off location

raw_data <- inner_join(raw_data,zone_lookup,by=c('DOLocation' = 'LocationID'))

drop <- c('Zone','service_zone','DOLocation')

raw_data <- raw_data[,!(colnames(raw_data) %in% drop)]

colnames(raw_data)[12] <- 'DOLocation'

```

##### Looking up Location names to the corresponding location ids such as 1-EWR, 2-Queens, 3- Bronx.

### 2.4.2 Calculated column of interest

```{r tip percentage,echo=FALSE}

# Adding tip percentage

raw_data['tip_perc'] <- c(round((raw_data$tip_amount/raw_data$fare_amount)*100,0))

# Adding trip duration

raw_data['trip_duration'] <- c(minute(seconds_to_period(raw_data$dropoff_time - raw_data$pickup_time))) +c(hour(seconds_to_period(raw_data$dropoff_time - raw_data$pickup_time))*60)

# Adding day of the week

raw_data['day'] <- c(wday(raw_data$pickup_time,label=TRUE))

```

```{r Pick up time period,echo=FALSE}

# Creating cut-off times

breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))

# Creating labels for cut off time

labels <- c("Night", "Morning", "Afternoon", "Evening")

# Calculating the time period based on the pick up time

raw_data['PU_time_of_day'] <- cut(x=hour(raw_data$pickup_time), breaks = breaks, labels = labels, include.lowest=TRUE)

# Changing the variable as categorical

raw_data$PU_time_of_day <- as.factor(raw_data$PU_time_of_day)

```

```{r Drop off time period,echo=FALSE}

# Calculating the time period based on the drop off time

raw_data['DO_time_of_day'] <- cut(x=hour(raw_data$dropoff_time), breaks = breaks, labels = labels, include.lowest=TRUE)

# Changing the variable as categorical

raw_data$DO_time_of_day <- as.factor(raw_data$DO_time_of_day)

```

##### Calculating columns of interest such as Trip duration, Trip percentage and Day

### 2.4.3 Missing vaules

```{r,echo=FALSE}

# Summary for NA values

xkabledply(data.frame(colSums(is.na(raw_data))),bso='bordered', title = 'NA Summary Pre-Cleaning', pos = 'center')

# Investigating Tip percentage

raw_data[is.na(raw_data$tip_perc),]

# On Investigating, it is found that there are trips with no fare amount and, by extension, no tips. Hence we will drop these values.

raw_data <- raw_data[!is.na(raw_data$tip_perc),]

# Investigating Passenger Count and RatecodeID

raw_data[is.na(raw_data$passenger_count),]

# We can drop records for which the number of passengers was not recorded, as this will tamper with our analysis of fare amount and tip percentage.

raw_data <- raw_data[!is.na(raw_data$passenger_count),]

xkabledply(data.frame(colSums(is.na(raw_data))),bso='bordered', title = 'NA Summary Post-Cleaning', pos = 'center')
```

##### Dealing with missing values

### 2.4.4 Defining categorical variables

```{r,echo=FALSE}

colnames(raw_data)

raw_data$VendorID <- as.factor(raw_data$VendorID)

raw_data$RatecodeID <- as.factor(raw_data$RatecodeID)

raw_data$payment_type <- as.factor(raw_data$payment_type)

raw_data$PULocation <- as.factor(raw_data$PULocation)

raw_data$DOLocation <- as.factor(raw_data$DOLocation)

raw_data$day <- as.factor(raw_data$day)


```

##### Defining variables such as vendor id ,passenger count, pick-up and Drop-off location as Categorical Variables.

### 2.4.5 Outliers

```{r Cleaning Data,echo=FALSE}

# Glimpse into the summary for the raw data.

xkablesummary(raw_data,bso='bordered', title = 'Summary of Raw Data', pos = 'center')

# Looking at the above summary, following observations are considered while dealing with outliers.
# 1. Passenger_count seems to go up to nine, which seems incorrect; hence, we will consider passengers up to 6.
# 2. The trip distance has a maximum value of 184341 miles, more than the entire United States. Therefore we consider trip distances up to 40 miles.
# 3. RatecodeID, according to our data, can be only six values; however, the data contains values beyond six. These values are neglected.
# 4. The fare amount ranges from -907 to 395845. Unless someone is too generous, these values are incorrect. The range for fare amount is considered from 0 to 150.
# 5. Similarly considering the range 0-100 for the tipping amount and toll collected.
# 6. The unknown values for the pick-up and drop-off locations are dropped.
# 7. When considering the payment type, only credit card payments are referenced.
# 8. Finally, values beyond the 500% tipping percentage looks skeptical; hence we will ignore these values and consider up to a 60% tipping ratio.

# Categorical variables check.

unique(raw_data$VendorID)
unique(raw_data$PULocation)
unique(raw_data$DOLocation)
unique(raw_data$RatecodeID)

# Numeric Variables check

raw_data[raw_data$tip_perc > 100,]

max(raw_data$fare_amount)
min(raw_data$fare_amount)
raw_data[raw_data$fare_amount > 150,]

max(raw_data$trip_distance)
min(raw_data$trip_distance)
raw_data[raw_data$trip_distance > 125,]

max(raw_data$tip_amount)
min(raw_data$tip_amount)
raw_data[raw_data$tip_amount > 80,]

max(raw_data$tolls_amount)
min(raw_data$tolls_amount)
raw_data[raw_data$tolls_amount > 80,]

raw_data[raw_data$PULocation == 'Unknown',]

raw_data[raw_data$DOLocation == 'Unknown',]

max(raw_data$tip_perc)
min(raw_data$tip_perc)
raw_data[raw_data$tip_perc > 100,]

# cleaning the final data

clean_data <- 
  raw_data %>% filter(
  (RatecodeID != 99)  		  & 
  (payment_type == 1)       &
  (PULocation != 'Unknown') & 
  (DOLocation != 'Unknown') &
  (passenger_count > 0) 	  & (passenger_count <= 6) &
  (trip_distance > 0) 		  & (trip_distance <150) 	& 
  (fare_amount > 0) 		    & (fare_amount < 150) 	& 
  (tip_amount > 0) 			    & (tip_amount < 100) 	  & 
  (tolls_amount > 0) 		    & (tolls_amount < 100) 	&  
  (trip_duration > 0) 		  & (trip_duration <= 40) & 
  (tip_perc >0) 			      & (tip_perc < 60))

nrow(clean_data)

```

```{r,echo=FALSE}
#clean_data
sum(clean_data$tip_perc,na.rm = TRUE)
```

##### **Investigating Outliers:** 
##### Looking at the above summary, following observations are considered while dealing with outliers:

1. Passenger_count seems to go up to nine, which seems incorrect; hence, we will consider passengers up to 6.
2. The trip distance has a maximum value of 184341 miles, more than the entire United States. Therefore we consider trip distances up to 40 miles.
3. Rate codeID, according to our data, can be only six values; however, the data contains values beyond six. These values are neglected.
4. The fare amount ranges from -907 to 395845. Unless someone is too generous, these values are incorrect. The range for fare amount is considered from 0 to 150.
5. Similarly considering the range 0-100 for the tipping amount and toll collected.
6. The unknown values for the pick-up and drop-off locations are dropped.
7. When considering the payment type, only credit card payments are referenced because we don't have data for cash tips.
8. Finally, values beyond the 500% tipping percentage looks skeptical; hence we will ignore these values and consider up to a 60% tipping ratio.

```{r,echo=FALSE}
xkabledply(data.frame(colSums(is.na(clean_data))),bso='bordered', title = 'NA Summary Post-Cleaning', pos = 'center')
```

**Summary for Cleaned Data**

```{r Summary for Cleaned Data, results='markup',echo=FALSE}

summary(clean_data)
```

##### The number of observations post data cleaning are ** `r nrow(clean_data)*ncol(clean_data)` **

## 2.5 Distribution Check

```{r Tip Percentage,echo=FALSE}

## The data is approximately normally distributed with slight skewness on the right side.

clean_data %>%
ggplot(aes(tip_perc)) +
  geom_histogram(aes(y =..density..),  colour = "black", fill = "#6a73b6", binwidth = 1) + 
  ggtitle("Distribution of NYC Taxi Tips") +
  stat_function(fun = dnorm, args = list(mean = mean(clean_data$tip_perc), sd = sd(clean_data$tip_perc))) +       
  labs(x= 'Tips', y='Density')

```

##### The distribution of the primary candidate for this study (Tip) is virtually normally distributed, with the lack of some value on the left side.

# 3 Explanatory Data Analysis

## 3.1 Parameter Visualization 

```{r Day Duration,echo=FALSE}

## It looks like the day of the week has no significant impact on the tip percentage.

trips_per_day <- clean_data %>% group_by(day) %>% count()

colnames(trips_per_day)[1] <- 'day'

#ggplot(data = trips_per_day, aes(y = n,x=day, fill = day)) +
  #geom_col() +  
  #labs (x= "Day of the Week", y = "No. of Trips") +
   #theme(plot.title = element_text(hjust = 0.5)) +
  #ggtitle('Day-wise Distribution of Trips')
```




```{r Vendor Split,echo=FALSE}

## Verifone has a greater number of market share

vendor_split <- clean_data %>% group_by(VendorID) %>% count()


# ggplot(vendor_split, aes(x = "", y = n, fill = VendorID)) +
#   geom_col(color = "black") +
#   geom_label(aes(label = n),
#              position = position_stack(vjust = 0.5),
#              show.legend = FALSE) +
#   coord_polar(theta = "y") +
#   scale_fill_manual(values = c("#5A5A5A","#0674C4"),labels=c('Creative Mobile Technologies', 'VeriFone'),name = "Vendor")+
#   theme(axis.text = element_blank(),
#         axis.title.x = element_blank(),
#         axis.title.y = element_blank(),
#         axis.ticks = element_blank(),
#         panel.grid  = element_blank(),
#         plot.title = element_text(hjust = 0.5)) +
#   ggtitle('Vendor Split')
```


```{r Passenger Trips Count,echo=FALSE}

## The plot is right skewed signifying there are greater number of trips with single passenger.

trips_per_passenger <- clean_data %>% group_by(passenger_count) %>%  count()

ggplot(trips_per_passenger,aes(passenger_count, n, fill = passenger_count)) +
  geom_col() +
  labs(x = "Number of passengers",y ="Total number of trips") +
  scale_fill_gradient(low="#F7BA2C",high="#F3696E") +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "none") +
  ggtitle('Trips by Passenger Count ')
summary(trips_per_passenger)
```

##### The graph above indicates that the number of journeys increases as the number of passengers decreases. The right-skewed graph corroborates this observation.

```{r Distribution for day wise trip count,echo=FALSE}

# select unique entries in the time of day 
lvls <- unique(unlist(clean_data$PU_time_of_day))

# Applying function which segregates the lvls value and count them in each col of dataset. Finally we choose 16 - PU_time_of_day and 17 - DO_time_of_day from the results.

time_period <- sapply(clean_data,function(x) table(factor(x, levels = lvls)))[,16:17]

# Converting index into first column 

time_period <- cbind(dayPeriod = rownames(time_period), time_period)
rownames(time_period) <- 1:nrow(time_period)

# Converting matrix as data frame

time_period <- as.data.frame(time_period)

# Converting variables as categorical for ploting

time_period['PU_time_of_day'] <- as.numeric(unlist(time_period['PU_time_of_day']))
time_period['DO_time_of_day'] <- as.numeric(unlist(time_period['DO_time_of_day']))
time_period['dayPeriod'] <- as.factor(noquote(time_period[,'dayPeriod']))

# For ploting two variables 

df <- melt(time_period, id.vars='dayPeriod')

ggplot(df, aes(x=dayPeriod, y=value, fill=variable)) + 
    geom_bar(stat = "identity",position = "dodge") +
  labs(x = "Time of the Day",y ="Total number of trips") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#5A5A5A","#0674C4"),labels=c('Pick Up Time', 'Drop Off Time'),name = "Time Category") +
  ggtitle('Distribution of time of the day')
```

##### We had expected that evenings would have the most travels, but our data revealed that afternoons were the busiest in terms of number of trips, followed by mornings. 

## 3.2 Relationship Exploration 

```{r Relationship Exploration,echo=FALSE}

str(clean_data)

melt(cor(clean_data[,unlist(lapply(clean_data, is.numeric))])) %>%
ggplot(aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  geom_text(aes(Var2, Var1, label = round(value,3)), color = "black", size = 2.5) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  axis.text.x = element_text(angle = 45, hjust=1),
  legend.justification = c(0, 1),
  plot.title = element_text(hjust = 0.5))+
  guides(fill = guide_colorbar(barwidth = 1, barheight = 5,
                title.position = "top", title.hjust = 0.5)) +
  scale_fill_manual(name = "Scale") +
  scale_fill_gradient(low="#fff33b",high="#e93e3a") +
  ggtitle('Correlation between Entities')  

```

##### **Observations** - We initially skimmed correlation coefficients for our continuous variables, such as travel distance and trip time, to see if they were connected to tip amount. As seen in the above cor plot, the results were 0.65 and 0.33, which suggest a moderate association. However, since correlation does not imply causation, statistical tests must be performed on these variables to establish their relationship.

## 3.3 Location Analysis

```{r Location wise distribution,echo=FALSE}
lvls <- unique(unlist(clean_data$PULocation))

location_dist <- sapply(clean_data,function(x) table(factor(x, levels = lvls)))[,11:12]

location_dist <- cbind(location = rownames(location_dist), location_dist)
rownames(location_dist) <- 1:nrow(location_dist)

location_dist <- as.data.frame(location_dist)

location_dist['PULocation'] <- as.numeric(unlist(location_dist['PULocation']))
location_dist['DOLocation'] <- as.numeric(unlist(location_dist['DOLocation']))
location_dist['location'] <- as.factor(noquote(location_dist[,'location']))

# For ploting two variables 

df <- melt(location_dist, id.vars='location')

ggplot(df, aes(x=location, y=value, fill=variable)) + 
    geom_bar(stat = "identity",position = "dodge") +
  labs(x = "Location",y ="Total number of trips") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#5A5A5A","#0674C4"),labels=c('Pick Up Location', 'Drop Off Location'),name = "Location Category") +
  ggtitle('Distribution of Location')
summary(location_dist)
```
```{r,results='markup',echo=FALSE}
#location_freq_dis <-  clean_data %>% group_by(PULocation,DOLocation) %>%  count(sort = T)

location_freq_dis <- clean_data %>% group_by(PULocation,DOLocation) %>%  count(sort = T)

avg_fare <- clean_data %>% group_by(PULocation,DOLocation) %>%  summarise(sum(fare_amount))

avg_fare <- inner_join(location_freq_dis,avg_fare,by=c('PULocation' = 'PULocation', 'DOLocation' = 'DOLocation'))

location_freq_dis['Avg. Fare'] <- avg_fare['sum(fare_amount)'] / avg_fare['n']

colnames(location_freq_dis)[1] <- 'Pick up Location'
colnames(location_freq_dis)[2] <- 'Drop off Location'
colnames(location_freq_dis)[3]   <- 'No. of Trips'

xkabledply(head(location_freq_dis,n=10),bso = 'bordered',title = 'Location Distribution')


```

##### Now that we have looked at the insights from our location vs tip percentage data, we explore the statistical significance of the two 


```{r,echo=FALSE}

ggplot(data = clean_data, aes(x = PULocation, fill = PULocation)) + geom_bar() + ggtitle("Trip Counts Based on Pick UP Location") + labs(x = "Location", y ="Trip Frequency") + theme(plot.title = element_text(hjust = 0.5)) 

ggplot(data = clean_data, aes(x = DOLocation, fill = DOLocation)) + geom_bar() + ggtitle("Trip Counts Based on Dropoff Location") + labs(x = "Location", y ="Trip Frequency") + theme(plot.title = element_text(hjust = 0.5)) 
```

##### This graph illustrates that Queens has the most tipping passengers, followed by Manhattan.Similarly, Manhattan has higher tipping passengers than others in Drop-Off Location

```{r ANOVA Tests for Location, results='markup',echo=FALSE}

anova_puloc_tip <- aov(tip_amount ~ PULocation, data = clean_data)
anova_doloc_tip <- aov(tip_amount ~ DOLocation, data = clean_data)

summary(anova_puloc_tip)
summary(anova_doloc_tip)

```

##### **Observations** - The p-value for both variables is 0.2*10^−15 or 0.0000000000000002, which are infinitesimal compared to the significance level of 0.05, thus rejecting the null hypothesis that the means of the two entities are the same, making them statically different.

## 3.4 Trip Duration Impact on Tips

```{r Trip Duration,echo=FALSE}

## The data is approximately normally distributed with slight skewness on the right side.

clean_data %>%
ggplot(aes(trip_duration)) +
  geom_histogram(aes(y =..density..),  colour = "black", fill = "#6a73b6", binwidth = 1) + 
  ggtitle("Distribution of NYC Taxi Ride Lenghts") +
  stat_function(fun = dnorm, args = list(mean = mean(clean_data$trip_duration), sd = sd(clean_data$trip_duration))) +       
  labs(x= 'Ride Durations', y='Density') 

```


##### **Observations** - The data is approximately normally distributed with slight skewness on the right side. This is to say that majority of out trips are 20-40 mins in length.



```{r ttest on tip and trip duration,echo=FALSE}

ttest_tip_duration <- t.test(clean_data$tip_perc,clean_data$trip_duration)

ttest_tip_duration
```

##### **Observations** - A t-test between travel time and customer tip percentage reveals the p-value of the relationship between the variables, which is 0.2*10-15; consequently, since the value is much lower than the significance level of 0.05, we can state that Yellow taxi passengers tip differently depending on the length of the trip and successfully reject the null hypothesis that the means of the two variables are equal.

## 3.5 Trip Length and Tips

#### We are attempting to determine if those doing shorter journeys are more likely to leave larger gratuities or those taking longer travels are more giving.

```{r,echo=FALSE}
# Subsetting columns of intrest

trip_len_nature <- subset(clean_data,select = c("tip_amount","trip_distance"))

# We begin by splitting up out data into two segments - Short Trips (0-20 mins) and Long Trips (20-40 mins)

breaks=c(0, mean(clean_data$trip_distance),max(clean_data$trip_distance))

# Creating labels for cut off time

labels <- c("Short Trip", "Long Trip")

# Splitting the trips into segment 

trip_len_nature['Duration_Type'] <- cut(x=as.integer(clean_data$trip_distance), breaks = breaks, labels = labels, include.lowest=TRUE)

trip_len_nature
# Checking for NA values in the new col

trip_len_nature[is.na(trip_len_nature),]

trip_len_nature %>% group_by(trip_distance,tip_amount,Duration_Type) %>% count() %>%
ggplot(aes(x = trip_distance, y = n, fill= Duration_Type)) + 
  geom_col(width = 0.2, alpha = 1) +
  labs(y = 'No of time tiped', x = 'Trip Distance', title = 'Distribution of tip percentages across trip Distance') +
  theme(plot.title = element_text(hjust = 0.5)) +
   scale_fill_manual(values = c("#003f5c","#ffa600"),labels=c('Short Trip', 'Long Trip'),name = "Trip Type") 

```


##### To study these two groups separately, we divide the data for trip distance into two categories: short and long trips. When we plot the journey distance against the number of tips paid, we notice that passengers tips higher number of times on shorter rides than on longer ones.



```{r,echo=FALSE}

melt(trip_len_nature) %>%
ggplot( aes(x = Duration_Type, y = value,fill = Duration_Type))+
  geom_boxplot() +
  stat_summary(fun.y=mean, geom="point", shape=23, size=4) + 
  labs(y = 'Tip Percentage', x = 'Distance Travelled', title = 'Box Plot of tip percentages vs trip Distance') +
  scale_fill_manual(values = c("#003f5c","#ffa600"),labels=c('Short Trip', 'Long Trip'),name = "Trip Type")
  
```


```{r,echo=FALSE}
st <- trip_len_nature[trip_len_nature$Duration_Type == 'Short Trip',] 
dt <- trip_len_nature[trip_len_nature$Duration_Type == 'Long Trip',]
  
str(st)
str(dt)

xkablesummary(st)
xkablesummary(dt)

```

```{r,echo=FALSE}

# A Simple two way test for pvalues which is found to be way less than the significant level 0.05 he

ttest_dis_tip <- t.test(dt$tip_amount , st$tip_amount)

ttest_dis_tip
```


##### **Observations** - A Simple two way test for pvalues which is found to be way less than the significant level 0.05. We can reject the null hypothesis, Z-test cannot be used because we don't know population's mean & std dev.

##### **Declaring hypothesis**

##### Null Hypothesis: Ho Tip amount is same for both short and long distance passenger(s)
##### Alternate Hypothesis: Ha Tip amount is NOT same for both short and long distance passenger(s)

 

## 3.6 Importance of passenger count and vendor

```{r,echo=FALSE}

tip_count_pass <- clean_data %>% group_by(passenger_count) %>% summarise_at(vars(tip_amount), list(count = length))

ggplot(data = tip_count_pass, aes(y = count,x=passenger_count, fill = passenger_count)) +
  geom_col() +  
  labs (x= "Number of Passengers", y = "Tips' Amount") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle('Passenger-wise Distribution of Tips')

```


```{r,echo=FALSE}

anova_tip_pass <- aov(tip_perc ~ passenger_count , data= clean_data)

summary(anova_tip_pass)
```


##### A Anova test between tip percentage and passenger count shows a significant relationship between the number of passengers and the amount of tips because the p-value is 0.00006, which is less than the significant value(0.005). Hence, we can reject the null hypothesis(H0).



```{r,echo=FALSE}

ttest_vendor = t.test(tip_perc ~ factor(VendorID), data = clean_data)

ttest_vendor
```

## 3.7 Summary


Feature (variable)  |  Test  |  P-value  | Null Hypothesis (H0)  | Decision on H0 |  
--------|-----|-----|--------|--------|  
 pickup location | ANOVA | `r format(summary(anova_puloc_tip)[[1]][["Pr(>F)"]][[1]], digits= 3 )` | means are equal | reject H0 |  
 dropoff location | ANOVA | `r format(summary(anova_doloc_tip)[[1]][["Pr(>F)"]][[1]], digits = 9)` | means are equal | reject H0 |  
 distance | T-Test | `r format(ttest_dis_tip$p.value, digits= 9 )` | means are equal | reject H0 |  
 passenger count | ANOVA | `r format(summary(anova_tip_pass)[[1]][["Pr(>F)"]][1], digits= 3)` | means are equal | reject H0 |
 vendor ID | T-test | `r format(ttest_vendor$p.value, digits= 3)` | means are equal | failed to reject H0 | 
 
 
```{r Reading Data, results='markup',echo=FALSE}

# Loading main data for New York yellow taxi trips
dataset <- data.frame(read.csv('../Data/Yellow_Taxi_Trips_Data_Raw_Cleaned.csv'))
```

```{r difining data types, echo=FALSE}

# Converting Variables into Categorical Variables

colnames(dataset)

dataset$VendorID <- as.factor(dataset$VendorID)
dataset$passenger_count <- as.factor(dataset$passenger_count)
dataset$RatecodeID <- as.factor(dataset$RatecodeID)
dataset$PULocation <- as.factor(dataset$PULocation)
dataset$DOLocation <- as.factor(dataset$DOLocation)
dataset$day <- as.factor(dataset$day)
dataset$PU_time_of_day <- as.factor(dataset$PU_time_of_day)
dataset$DO_time_of_day <- as.factor(dataset$DO_time_of_day)

# Removing Payment type as we don't need it as all the tips considered are through credit card

dataset <- within(dataset, rm(payment_type))

```


# 4 Model Building

We began by selecting linear (univariate and multivariate) regression models to examine how they fit our data. Linear regression is a conventional, common approach that may explain the association with tip well, so we chose to test it first. To strengthen our linear model, we also used lasso, ridge, and principal component analysis (PCA). We also made use of decision trees and random forest as regression. The capacity of decision trees to mimic non-linear connections is one of its advantages. According to our EDA, journey duration, distance, and fare are all linearly connected over small distances, but this connection weakens over longer distances due to the involvement of other possible factors.Consequently, there may be in fact a non-linear relationship with tip, too.

## 4.1 Preparation

We prepped our data for modeling before developing our models by using one hot encoding, establishing training and testing sets, and scaling our data.

### 4.1.1 One hot encoding (OHE)

We employed one hot encoding to convert factor columns to numerical columns. All factors will be converted into a distinct boolean column by a one hot encoding.

```{r OHE, echo=FALSE}

colnames(dataset)

# Extracting categorical vairables

fact <- c("VendorID","passenger_count","RatecodeID","PULocation", "DOLocation","day","PU_time_of_day","DO_time_of_day")

dataset <- dummy_cols(dataset, select_columns = fact)

# Removing the orignal vairable

dataset <- dataset[,!names(dataset) %in% fact]

```
```{r after ohe, echo=FALSE,results='markup'}
glimpse(dataset)
```


Post One Hot Encoding (OHE) we are now left with **`r dim(dataset)[2]` columns**. 

### 4.1.2 Scaling variables

Because the magnitude of the values may not be proportionate, we must scale the numerical variables in our datasets. For comparative reasons, we compute the mean and standard deviation of each numerical column.

```{r Scaling,echo=FALSE}
nums <- c('trip_distance','fare_amount','tip_amount','tolls_amount')

# Mean and SD of numerical columns

build_scales(data_set = dataset, cols = nums, verbose = TRUE)

dataset <- dataset %>%           # Applying functions of dplyr
  mutate_at(nums, ~(scale(.) %>% as.vector))


```

### 4.1.3 Test train split

In order to eliminate any bias in test results while utilizing train data, the train-test split should be implemented before (most) data modeling. We randomly divided the dataset into 70% train and 30% test to replicate a train and test set.

```{r Spliting, echo=FALSE}
set.seed(1)

#train and test split (Since we have enough data with can split our data into train (80%) & test (20%))
sample <- sample(c(TRUE, FALSE), nrow(dataset), replace=TRUE, prob=c(0.7,0.3))
train  <- dataset[sample, ]
test   <- dataset[!sample, ]

colnames(train)[colnames(train) == 'PULocation_Staten Island'] = "PULocation_Staten_Island"
colnames(train)[colnames(train) == 'DOLocation_Staten Island'] = "DOLocation_Staten_Island"
colnames(test)[colnames(test) == 'PULocation_Staten Island'] =  "PULocation_Staten_Island"
colnames(test)[colnames(test) == 'DOLocation_Staten Island'] = "DOLocation_Staten_Island"

```

Number of rows of observations in training dataset is **`r nrow(train)`** and in testing dataset are **`r nrow(test)`** post split.

## 4.2 Evaluation Metrics

*From Wikipedia*

1. **Mean Absolute Percentage Error (MAPE)**

|               The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction 
|               accuracy of a forecasting method in statistics. It usually expresses the accuracy as a ratio defined by the formula:

|               \n![](Mape.svg)

2. **Akaike information criterion (AIC)**

|               The Akaike information criterion is an estimator of prediction error and thereby relative quality of statistical models for a 
|               given set of data.Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the  
|               other models.:

|               \n![](AIC.svg)

3. **R squared (R^2^)**

|               In statistics, the coefficient of determination, denoted R2 or r2 and pronounced "R squared", is the proportion of the variation 
|               in the dependent variable that is predictable from the independent variable(s).

|               \n![](R-squared.svg)

## 4.3 Tip Percentage

The subsequent step post preparing our data is to employ a number of regression-based methods to extract insights from data, which we can then use to predict which result is likely to hold true for our target variable based on training data.

### 4.3.1 Principal component analysis

We chose PCA as a variable reduction strategy because the majority of our variables were associated with one another and there were 48 features.

```{r pca,echo=FALSE}

# get principal components using prcomp, we have used scale = F as the data is already scaled
pr.out =prcomp(train[,-c(7,4,5)] %>% select_if(is.numeric), scale = F)
summary(pr.out)

fviz_eig(pr.out, addlabels = TRUE)

```




```{r,echo=FALSE}
fviz_pca_var(pr.out,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

**Variables graph:**  Variables that are positively associated point to the same side of the plot. Negatively associated variables point to the graph's opposing sides.

**Observations:** Even if the fist three components explain 94.1% of the variance in the data, that may not necessarily mean that a good R^2^ or high coefficients will result. However this gives us enough statistical basis for which variables to go after. Hence, we proceed to build out Linear regression model with these variables.

### 4.3.2 Linear regression

From the results of the Principle component analysis, we constructed a linear model with tip percentage along with the first three high variability explainers and correlated values for 'Tip Percentage' - trip_distance (-0.28), trip_duration (-0.175), and toll_amount (-0.04).

```{r Linear Models,echo=FALSE}
#Taking best three variables correlated with tip perc based on Pearson Correlation

#linear model of tip_perc ~ trip_duration
fit1 <- lm(tip_perc ~ trip_duration, data = train )
#summary(fit1)
# vif(fit1)
# plot(fit1)


#linear model of tip_perc ~ trip_duration+fare_amount
fit2 <- lm(tip_perc ~ trip_duration+tolls_amount, data = train )
#summary(fit2)
# vif(fit2)
# plot(fit2)


#linear model of tip_perc ~ trip_duration+fare_amount+trip_distance
fit3 <- lm(tip_perc ~ trip_duration+tolls_amount+trip_distance, data = train )
#summary(fit3)
#vif(fit3)
#plot(fit3)

plot(fit1)
plot(fit2)
plot(fit3)
```

```{r AIC of Linear Models, echo=FALSE}
#define list of models
models <- list(fit1, fit2, fit3)

#specify model names
mod.names <- c('trip_duration.trip_duration.fare_amount.trip_distance', 'trip_duration.trip_duration.fare_amount', 'trip_duration.trip_duration')

#calculate AIC of each model
AIC <- aictab(cand.set = models, modnames = mod.names)
```

```{r,echo=FALSE}
options(scipen=999)
```

**ANOVA tests on all the three models**

The summary for three linear models is:

 Fit | Model Equation | R^2 | ANOVA P-value | AIC |  
 ----|----------------------|-----|-----------|-----|  
 1   | tip_perc ~ trip_duration | `r format(summary(fit1)$r.squared, digits=3)` | -- | `r AIC[,3][1]` |
 2   | tip_perc ~ trip_duration+fare_amount |`r format(summary(fit2)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][2])` | `r AIC[,3][2]` |
 3   | tip_perc ~ trip_duration+fare_amount+trip_distance |`r format(summary(fit3)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][3], digits=3)` | `r AIC[,3][3]` |

**Observations:** Looking at the combination of p-value and r-squared, we conclude that fit3 performs slightly better than the other two fits. Hence, we check if there is an improvement in model 3 in the absence of outliers.

```{r, include=TRUE, echo=FALSE}
#Prediction

model.final.pred <- add_predictions(test,fit3)

# Model performance metrics

results_df <- data.frame(
  technique = 'Linear(3 vars with best cor-coeffs)',
  dependent = 'tip_perc',
  mape = MAPE(model.final.pred$tip_perc, model.final.pred$pred),
  Rsquare = caret::R2(model.final.pred$pred, model.final.pred$tip_perc),
  AIC = AIC[,3][3]
)


```

##### **Treating Outliers and Modeling**

```{r Removing Outliers, echo=FALSE}

train_no <- train

outlierKD2(train_no,var = trip_duration,boxplt = T,rm = T)
outlierKD2(train_no,var = fare_amount,boxplt = T,rm = T)
outlierKD2(train_no,var = trip_distance,boxplt = T, rm = T)

model_no <- lm(tip_perc ~ trip_duration+tolls_amount+trip_distance, data = train_no)

summary(model_no)

predictions <- predict(model_no, test)
```

```{r AIC of Linear Models (NO), echo=FALSE}

#define list of models
models <- list(model_no)

#specify model names
mod.names <- c('Model 3 without outliers')

#calculate AIC of each model
AIC <- aictab(cand.set = models, modnames = mod.names)

```

```{r, echo=FALSE}
# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear-treated outlier",
  dependent = "tip_perc",
  mape = MAPE(test$tip_perc, predictions),
  Rsquare = caret::R2(predictions, test$tip_perc),
  AIC = AIC[,3]
))

```

Even after treating the outliers in our model three fit, There is little to no difference in the results, and the r-squared and MAPE values remains `r format(results_df[2, 4], digits=3)` and `r format(results_df[2, 3], digits=4)`.

### 4.3.3 Lasso regression

Lasso regression is a form of regularization (L1) approach that might result in coefficients that are canceled out (in other words, some of the features are completely neglected for the evaluation of output). As a result, it not only helps to reduce over-fitting, but it may also aid in feature selection.

As we increase the value of lambda, the bias increase and  variance decrease, so we Iterated through a set of lambda values to find the optimum value. The graph below shows how lasso reduces the value of unnecessary attribute coefficients to 0. Only the five attributes with the greatest coefficient values are indicated for greater visibility

```{r lasso_tipperc, echo=FALSE}


# Prep model parameters
x=model.matrix(tip_perc~.-tip_perc-fare_amount-tip_amount-dropoff_time-pickup_time,train)[,-1]

y = train %>%
  select(tip_perc) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using 10 cross-validation
set.seed(123) 

#Note that setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. 

lasso_cv <- cv.glmnet(x, y, alpha = 1, standardize = FALSE)

# Display the best lambda value
best_alpha <- lasso_cv$lambda.min


# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_alpha, standardize = FALSE)

# # Make predictions on the test data
x.test <- model.matrix(tip_perc~.-tip_perc-tip_amount-fare_amount-dropoff_time-pickup_time, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()

#AIC 
tLL <- lasso_model$nulldev - deviance(lasso_model)
k <- lasso_model$df
n <- lasso_model$nobs

AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Lasso",
  dependent = "tip_perc",
  mape = MAPE(test$tip_perc, predictions),
  Rsquare = caret::R2(predictions, test$tip_perc),
  AIC = paste(AICc,'*')
))
```

```{r Lassocoeff,echo=FALSE}

# Dsiplay regression coefficients
coef(lasso_model)

```


```{r ,results='markup',echo=FALSE}
fit.lasso <- glmnet(x, y, family="gaussian", alpha=1)
plot_glmnet(fit.lasso, label=10)
```

It is interesting, that the **Trip Distance (Short and Long Trip)** and the **Standard Rate Applied to the Rides (Rate Code 1)** survived the longest. it also surprising to see how long did the **Drop off Location Bronx** prevails, which is understandable.

```{r LassoMSE , results='markup',echo=FALSE}

# Plot lambda CV
plot(lasso_cv)

```

The lambda value that minimizes the test MSE turns out to be **`r lasso_cv$lambda.min`** .

```{r LassoPredictions, results='markup', echo=FALSE}
# Plot predicted vs actual values
ggplot(test, aes(x =test$tip_perc ,y = predictions)) +
  geom_point() +
  labs(title="Predicted vs Actual Values",
       x="Actual Tip Percentage  ", y = "Predicted Tip Percentage")
```

There is a slight improvement in the r-sqaured and mape value in comparison to the base linear model, however the r-squared is only `r format(results_df[3, 4], digits=3)`, which means there is room for a lot improvement.

### 4.3.4 Ridge regression

The coefficients that best suit the data are discovered using the least squares approach. It should also determine the unbiased coefficients as a further requirement. Here, unbiased refers to the fact that OLS ignores the independent variables' relative importance. A given data set's coefficients are easily found. In other words, the lowest "Residual Sum of Squares (RSS)" can only be obtained from one set of betas. It therefore poses a question whether the model with the lowest RSS is actually the better model.

In a sense, OLS offers the model with the highest variance and the lowest bias, and it gets more complex as the number of variables rises. Although it is stationary and never moves, we still want a model with little bias and little variance. This void can be filled by Ridge, which is also known as regularization. Since the ridge regression penalizes coefficients, the least effective ones in the estimation will "shrink" the quickest. In ridge regression, the lambda parameter (penalizing factor) can be adjusted to alter the model coefficients.

Again, Only the five attributes with the greatest coefficient values are indicated for greater visibility.

```{r ridge_tipperc, echo=FALSE}

# Prep model parameters
x=model.matrix(tip_perc~.-tip_perc-tip_amount-fare_amount-dropoff_time-pickup_time,train)[,-1]

y = train %>%
  select(tip_perc) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using 10 cross-validation
set.seed(123) 

#Note that setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. 

ridge_cv <- cv.glmnet(x, y, alpha = 0, standardize = FALSE)

# Display the best lambda value
best_alpha <- ridge_cv$lambda.min

# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = best_alpha, standardize = FALSE)

# # Make predictions on the test data
x.test <- model.matrix(tip_perc~.-tip_perc-tip_amount-fare_amount-dropoff_time-pickup_time, test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

#AIC 
tLL <- ridge_model$nulldev - deviance(ridge_model)
k <- ridge_model$df
n <- ridge_model$nobs

AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)


# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Ridge",
  dependent = "tip_perc",
  mape = MAPE(test$tip_perc, predictions),
  Rsquare = caret::R2(predictions, test$tip_perc),
  AIC = paste(AICc,'*')
))

```

```{r Ridgecoeff,echo=FALSE}

# Dsiplay regression coefficients
coef(ridge_model)

```

```{r ,results='markup',echo=FALSE}
fit.ridge <- glmnet(x, y, family="gaussian", alpha=0)
plot_glmnet(fit.ridge, label=10)
```

**Observations: **The plot shows the whole path of variables as they shrink towards zero as lambda increases. The **Pick up location of Staten Island and Newark Airport** survives the longest as they shrink to zero.

```{r Ridgemse, results='markup',echo=FALSE}

# Plot lambda CV
plot(ridge_cv)

```

The lambda value that minimizes the test MSE turns out to be **`r ridge_cv$lambda.min`** .

```{r RidgePredictions, results='markup', echo=FALSE}
# Plot predicted vs actual values
ggplot(test, aes(x =test$tip_perc ,y = predictions)) +
  geom_point() +
  labs(title="Predicted vs Actual Values",
       x="Actual Tip Percentage  ", y = "Predicted Tip Percentage")

```


```{r plot_r2_ridge_ratio, include=TRUE, echo= FALSE}
# Plot graph for glmnet
fit = glmnet(x, y, alpha = 0)
plot(fit, xvar = "dev", label = TRUE)
results_df
```

The Plot shows that all the variables explain ~9.30% (~0.0930 point on the plot) of the variance in the data. Same is bolstered by the R^2^ value of the model.

### 4.3.5 Decision tree

The classification and regression tree (CART) methodology is one of the earliest methods for creating regression trees, however there are many more. A data set is divided into smaller subgroups by basic regression trees, which then fit a straightforward constant to each observation in each segment. By using successive binary partitions (also known as recursive partitioning) depending on several predictors, the partitioning is accomplished.

**Cost complexity criterion**

To enhance prediction performance on certain unknown data, a balance in the depth and complexity of the tree is generally required. To achieve this balance, we generally create a very big tree and then prune it back to identify an ideal subtree. We identify the best subtree by applying a cost complexity parameter (α) that penalizes our objective function for the number of terminal nodes in the tree.

```{r Decision tree, results='markup',echo=FALSE}

tip_perc_dt <- rpart(tip_perc ~ trip_distance + tolls_amount + trip_duration + VendorID_1 
+ VendorID_2 + passenger_count_1 + passenger_count_2 + passenger_count_3 + passenger_count_4 + passenger_count_5 + passenger_count_6 + RatecodeID_1 + RatecodeID_2 + RatecodeID_3 + RatecodeID_4 + RatecodeID_5 + PULocation_Bronx + PULocation_Brooklyn + PULocation_EWR + PULocation_Manhattan + PULocation_Queens + PULocation_Staten_Island + DOLocation_Bronx + DOLocation_Brooklyn +  DOLocation_EWR + DOLocation_Manhattan + DOLocation_Queens +  DOLocation_Staten_Island + day_Fri + day_Mon + day_Sat + day_Sun + day_Thu + day_Tue + day_Wed + PU_time_of_day_Afternoon + PU_time_of_day_Evening + PU_time_of_day_Morning + PU_time_of_day_Night + DO_time_of_day_Afternoon + DO_time_of_day_Evening + DO_time_of_day_Morning + DO_time_of_day_Night, data=train, method='anova',cp = 0,)

rpart.plot(tip_perc_dt,extra = 100,type = 1,snip = TRUE)
```

When we consider all the variables while building our decision tree, the model quickly becomes overfitted.

```{r,echo=FALSE}
plotcp(tip_perc_dt)
abline(v = 10, lty = "dashed")
```

The above shows the compares the error over the range of α's (cost complexity - cp value at the bottom X-axis). The upper X-axis gives the number of nodes. We can see returns diminish after around 10 leafs (dashed vertical line).

##### **Pruned Decision Tree**

Pruning the decision tree to 10 variables gives a much better model as seen below.

```{r Pruned Tree, results='markup',echo=FALSE}
tip_perc_pt <- rpart(tip_perc ~ trip_distance + tolls_amount + trip_duration + VendorID_1 
+ VendorID_2 + passenger_count_1 + passenger_count_2 + passenger_count_3 + passenger_count_4 + passenger_count_5 + passenger_count_6 + RatecodeID_1 + RatecodeID_2 + RatecodeID_3 + RatecodeID_4 + RatecodeID_5 + PULocation_Bronx + PULocation_Brooklyn + PULocation_EWR + PULocation_Manhattan + PULocation_Queens + PULocation_Staten_Island + DOLocation_Bronx + DOLocation_Brooklyn +  DOLocation_EWR + DOLocation_Manhattan + DOLocation_Queens +  DOLocation_Staten_Island + day_Fri + day_Mon + day_Sat + day_Sun + day_Thu + day_Tue + day_Wed + PU_time_of_day_Afternoon + PU_time_of_day_Evening + PU_time_of_day_Morning + PU_time_of_day_Night + DO_time_of_day_Afternoon + DO_time_of_day_Evening + DO_time_of_day_Morning + DO_time_of_day_Night, data=train, method='anova',control = c(cp = 0.001, maxdepth = 30))

rpart.plot(tip_perc_pt,extra = 100,type = 1,snip = TRUE)
```

```{r,echo=FALSE}
plotcp(tip_perc_pt)
```

The above plot confirms that only the first ten variables actually contributes towards reducing the relative error. 

```{r meterics,echo=FALSE}

# Original
# Predict test values using tree
tree_perc_pred = predict(tip_perc_dt, newdata = test)

# Obtain MSE
tree_perc_mse = (sum((tree_perc_pred - test$tip_perc)^2)) / nrow(test)

# Obtain MAPE and r2
mape_ratio = MAPE(test$tip_perc, tree_perc_pred)
Rsquare_ratio = caret::R2(tree_perc_pred, test$tip_perc)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree",
  dependent = "tip_perc",
  mape = mape_ratio,
  Rsquare = Rsquare_ratio,
  AIC = '--'
))

# Prune
# Predict test values using tree
tree_perc_pred_prune = predict(tip_perc_pt, newdata = test)

# Obtain MSE
tree_perc_mse_prune = (sum((tree_perc_pred_prune - test$tip_perc)^2)) / nrow(test)


# Obtain MAPE and r2
mape_perc_prune = MAPE(test$tip_perc, tree_perc_pred_prune)
Rsquare_perc_prune = caret::R2(tree_perc_pred_prune, test$tip_perc)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree (Prune)",
  dependent = "tip_perc",
  mape = mape_perc_prune,
  Rsquare = Rsquare_perc_prune,
  AIC = '--'
))

xkabledply(results_df, title = 'Grand Summary of All Models')

results_df

```

### 4.3.6 Random Forest

Finally, we apply our last model to further obtained enhanced results. The Random Forest builds on top of the classical decision tree by a method called **Bagging**

**Note: ** Due to limitation in computation power, the number of trees are limited to 100.

```{r Random Forest,results='markup', echo=FALSE}

rf.fit <- randomForest(tip_perc ~  trip_distance  + tolls_amount + trip_duration + VendorID_1 
+ VendorID_2 + passenger_count_1 + passenger_count_2 + passenger_count_3 + passenger_count_4 + passenger_count_5 + passenger_count_6 + RatecodeID_1 + RatecodeID_2 + RatecodeID_3 + RatecodeID_4 + RatecodeID_5 + PULocation_Bronx + PULocation_Brooklyn + PULocation_EWR + PULocation_Manhattan + PULocation_Queens + PULocation_Staten_Island + DOLocation_Bronx + DOLocation_Brooklyn +  DOLocation_EWR + DOLocation_Manhattan + DOLocation_Queens + DOLocation_Staten_Island + day_Fri + day_Mon + day_Sat + day_Sun + day_Thu + day_Tue + day_Wed + PU_time_of_day_Afternoon + PU_time_of_day_Evening + PU_time_of_day_Morning + PU_time_of_day_Night + DO_time_of_day_Afternoon + DO_time_of_day_Evening + DO_time_of_day_Morning + DO_time_of_day_Night, data = train, ntree=100,keep.forest=FALSE, importance=TRUE)

print(rf.fit)

```

```{r ploting results,echo=FALSE}
ImpData <- as.data.frame(importance(rf.fit))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )

```

Node purity is the total decrease in residual sum of squares when splitting on a variable averaged over all trees (i.e. how well a predictor decreases variance). Importance gives you what the model has learnt. The above plot shows, for each variable, how important it is in classifying the data. The Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance.

Since the results of the Random Forest was so low, we decided to exclude it from out model selection.

### 4.3.7 Summary and analysis

The unpruned decision tree is the optimal model when aiming for a low MAPE, low AIC, and high r-squared. It is crucial to remember that when comparing the different models, the MAPE number is typically the same, hovering around 0.18 - 0.22, which denotes a 78-82 % accuracy. But all of the models' r-squared values are quite low, explaining about 8% to 11% of the variance in our dependent variable. As a result, this suggests that the models are neither thorough nor accurate fits.


## 4.4 Tip Amount

The subsequent step post preparing our data is to employ a number of regression-based methods to extract insights from data, which we can then use to predict which result is likely to hold true for our target variable based on training data.

### 4.4.1 Principal component analysis

We chose PCA as a variable reduction strategy because the majority of our variables were associated with one another and there were 48 features.

```{r pca1,echo=FALSE}

# get principal components using prcomp, we have used scale = F as the data is already scaled
pr.out =prcomp(train[,-c(5,7)] %>% select_if(is.numeric), scale = F)
summary(pr.out)

fviz_eig(pr.out, addlabels = TRUE)

train

```


```{r,echo=FALSE}
fviz_pca_var(pr.out,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

**Variables graph:**  Variables that are positively associated point to the same side of the plot. Negatively associated variables point to the graph's opposing sides.

**Observations:** Even if the first three components explain 94% of the variance in the data, that may not necessarily mean that a good R^2^ or high coefficients will result. However this gives us enough statistical basis for which variables to go after. Hence, we proceed to build out Linear regression model with these variables.


### 4.4.2 Linear regression

From the results of the Principle component analysis, we constructed a linear model with tip amount along with the first three high variability explainers and correlated values for 'Tip_Amount': fare_amount (0.65), trip_distance (0.54), and trip_duration (0.36).

```{r Linear_Models1,echo=FALSE}
#Taking best three variables correlated with tip amount based on Pearson Correlation

#linear model of tip_amount ~ trip_duration
fit1 <- lm(tip_amount ~ trip_duration, data = train )
#summary(fit1)
# vif(fit1)
# plot(fit1)


#linear model of tip_amount ~ trip_duration+fare_amount
fit2 <- lm(tip_amount ~ trip_duration+fare_amount, data = train )
#summary(fit2)
# vif(fit2)
# plot(fit2)


#linear model of tip_amount ~ trip_duration+fare_amount+trip_distance
fit3 <- lm(tip_amount ~ trip_duration+fare_amount+trip_distance, data = train )
#summary(fit3)
#vif(fit3)
#plot(fit3)

plot(fit1)
plot(fit2)
plot(fit3)
```

```{r AIC_of_Linear_Models1, echo=FALSE}
#define list of models
models <- list(fit1, fit2, fit3)

#specify model names
mod.names <- c('trip_duration.trip_duration.fare_amount.trip_distance', 'trip_duration.trip_duration.fare_amount', 'trip_duration.trip_duration')

#calculate AIC of each model
AIC <- aictab(cand.set = models, modnames = mod.names)

```

**ANOVA tests on all the three models**

The summary for three linear models is:

 Fit | Model Equation | R^2 | ANOVA P-value | AIC |  
 ----|----------------------|-----|-----------|-----|  
 1   | tip_amount ~ trip_duration | `r format(summary(fit1)$r.squared, digits=3)` | -- | `r AIC[,3][1]` |
 2   | tip_amount ~ trip_duration+fare_amount |`r format(summary(fit2)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][2])` | `r AIC[,3][2]` |
 3   | tip_amount ~ trip_duration+fare_amount+trip_distance |`r format(summary(fit3)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][3], digits=3)` | `r AIC[,3][3]` |


**Observations:** Looking at the combination of p-value and r-squared, we conclude that fit3 performs slightly better than the other two fits. Hence, we check if there is an improvement in model 3 in the absence of outliers.

```{r, include=TRUE, echo=FALSE}
#Prediction

model.final.pred <- add_predictions(test,fit3)

# Model performance metrics

results_df <- rbind(results_df,data.frame(
  technique = 'Linear(3 vars with best cor-coeffs)',
  dependent = 'tip_amount',
  mape = MAPE(model.final.pred$tip_amount, model.final.pred$pred),
  Rsquare = caret::R2(model.final.pred$pred, model.final.pred$tip_amount),
  AIC = AIC[,3][3]
))
results_df

```

##### **Treating Outliers and Modeling**

```{r Removing_Outliers1, echo=FALSE}

train_no <- train

outlierKD2(train_no,var = trip_duration,boxplt = T,rm = T)
outlierKD2(train_no,var = fare_amount,boxplt = T,rm = T)
outlierKD2(train_no,var = trip_distance,boxplt = T, rm = T)

model_no <- lm(tip_amount ~ trip_duration+fare_amount+trip_distance, data = train_no)

summary(model_no)

predictions <- predict(model_no, test)
```

```{r AIC_of_Linear_Models1 (NO), echo=FALSE}

#define list of models
models <- list(model_no)

#specify model names
mod.names <- c('Model 3 without outliers')

#calculate AIC of each model
AIC <- aictab(cand.set = models, modnames = mod.names)

```

```{r, echo=FALSE}
# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear-treated outlier",
  dependent = "tip_amount",
  mape = MAPE(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount),
  AIC = AIC[,3]
))

```

Even after treating the outliers in our model three fit, There is little to no difference in the results, and the r-squared and MAPE values remains `r format(results_df[8, 4], digits=3)` and `r format(results_df[8, 3], digits=4)`.

### 4.4.3 Lasso regression


```{r lasso_tip_amount1, echo=FALSE}

# Prep model parameters
x=model.matrix(tip_amount~.-tip_amount-tip_perc-dropoff_time-pickup_time,train)[,-1]

y = train %>%
  select(tip_amount) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using 10 cross-validation
set.seed(123) 

#Note that setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. 

lasso_cv <- cv.glmnet(x, y, alpha = 1, standardize = FALSE)

# Display the best lambda value
best_alpha <- lasso_cv$lambda.min

# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_alpha, standardize = FALSE)

# # Make predictions on the test data
x.test <- model.matrix(tip_amount~.-tip_amount-tip_perc-dropoff_time-pickup_time, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()

#AIC 
tLL <- lasso_model$nulldev - deviance(lasso_model)
k <- lasso_model$df
n <- lasso_model$nobs

AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)


# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Lasso",
  dependent = "tip_amount",
  mape = MAPE(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount),
  AIC = paste( AICc, '*')
))
```

```{r Lassocoeff1 ,echo=FALSE}

# Dsiplay regression coefficients
coef(lasso_model)

```


```{r ,results='markup',echo=FALSE}
fit.lasso <- glmnet(x, y, family="gaussian", alpha=1)
plot_glmnet(fit.lasso, label=10)
```

As expected the **fare amount** survives the longest. However, it  surprising to see how long did the **Toll Amount** and **Drop Off Location Bronx** prevails.

```{r LassoMSE1 , results='markup',echo=FALSE}

# Plot lambda CV
plot(lasso_cv)

```

The lambda value that minimizes the test MSE turns out to be **`r lasso_cv$lambda.min`** .

```{r LassoPredictions1, results='markup', echo=FALSE}
# Plot predicted vs actual values
ggplot(test, aes(x =test$tip_amount ,y = predictions)) +
  geom_point() +
  labs(title="Predicted vs Actual Values",
       x="Actual Tip amount  ", y = "Predicted Tip amount")

```

As with before, the r-squared is around 0.4463, which means there is room for a lot improvement.


### 4.4.4 Ridge regression


```{r ridge_tip_amount1, echo=FALSE}

#reduced_train <- sample_n(train, 15000)
#reduced_test <- sample_n(test, 15000)

# Prep model parameters
x=model.matrix(tip_amount~.-tip_amount-tip_perc-dropoff_time-pickup_time,train)[,-1]

y = train %>%
  select(tip_amount) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using 10 cross-validation
set.seed(123) 

#Note that setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. 

ridge_cv <- cv.glmnet(x, y, alpha = 0, standardize = FALSE)

# Display the best lambda value
best_alpha <- ridge_cv$lambda.min

# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = best_alpha, standardize = FALSE)

# # Make predictions on the test data
x.test <- model.matrix(tip_amount~.-tip_amount-tip_perc-dropoff_time-pickup_time, test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

#AIC 
tLL <- ridge_model$nulldev - deviance(ridge_model)
k <- ridge_model$df
n <- ridge_model$nobs

AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)


# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Ridge",
  dependent = "tip_amount",
  mape = MAPE(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount),
  AIC = paste( AICc,'*')
))

```

```{r Ridgecoeff1,echo=FALSE}

# Dsiplay regression coefficients
coef(ridge_model)

```

```{r ,results='markup',echo=FALSE}
fit.ridge <- glmnet(x, y, family="gaussian", alpha=0)
plot_glmnet(fit.ridge, label=10)
```

**Observations: ** The plot shows the whole path of variables as they shrink towards zero as lambda increases. The **Pick up location of Staten Island** and **Nassau or Winchester (Rate Code 4)** survives the longest as they shrink to zero.


```{r Ridgemse1, results='markup',echo=FALSE}

# Plot lambda CV
plot(ridge_cv)

```

The lambda value that minimizes the test MSE turns out to be **`r ridge_cv$lambda.min`** .

```{r RidgePredictions1, results='markup', echo=FALSE}
# Plot predicted vs actual values
ggplot(test, aes(x =test$tip_amount ,y = predictions)) +
  geom_point() +
  labs(title="Predicted vs Actual Values",
       x="Actual Tip Amount  ", y = "Predicted Tip Amount")

```


```{r plot_r2_ridge_ratio1, include=TRUE, echo= FALSE}
# Plot graph for glmnet
fit = glmnet(x, y, alpha = 0)
plot(fit, xvar = "dev", label = TRUE)

results_df
```

The Plot shows that all the vairables explain ~42% (~0.4288 point on the plot) of the variance in the data. Same is bolstered by the R^2^ value of the model.

### 4.4.5 Decision tree


**Cost complexity criterion**


```{r Decision_tree1, results='markup',echo=FALSE}

tip_amount_dt <- rpart(tip_amount ~ trip_distance + fare_amount  + tolls_amount + trip_duration + VendorID_1 
+ VendorID_2 + passenger_count_1 + passenger_count_2 + passenger_count_3 + passenger_count_4 + passenger_count_5 + passenger_count_6 + RatecodeID_1 + RatecodeID_2 + RatecodeID_3 + RatecodeID_4 + RatecodeID_5 + PULocation_Bronx + PULocation_Brooklyn + PULocation_EWR + PULocation_Manhattan + PULocation_Queens + PULocation_Staten_Island + DOLocation_Bronx + DOLocation_Brooklyn +  DOLocation_EWR + DOLocation_Manhattan + DOLocation_Queens +  DOLocation_Staten_Island + day_Fri + day_Mon + day_Sat + day_Sun + day_Thu + day_Tue + day_Wed + PU_time_of_day_Afternoon + PU_time_of_day_Evening + PU_time_of_day_Morning + PU_time_of_day_Night + DO_time_of_day_Afternoon + DO_time_of_day_Evening + DO_time_of_day_Morning + DO_time_of_day_Night, data=train, method='anova',cp = 0,)

rpart.plot(tip_amount_dt,extra = 100,type = 1,snip = TRUE)
```

When we consider all the variables while building our decision tree, the model quickly becomes overfitted.

```{r,echo=FALSE}
plotcp(tip_amount_dt)
abline(v = 13, lty = "dashed")
```

The above shows the compares the error over the range of α's (cost complexity - cp value at the bottom X-axis). The upper X-axis gives the number of nodes. We can see returns deminish after around 13 leafs (dashed vertical line).

##### **Pruned Decision Tree**

Pruning the decision tree to 13 variables gives a much better model as seen below.

```{r Pruned_Tree1, results='markup',echo=FALSE}
tip_amount_pt <- rpart(tip_amount ~ trip_distance + fare_amount + tolls_amount + trip_duration + VendorID_1 
+ VendorID_2 + passenger_count_1 + passenger_count_2 + passenger_count_3 + passenger_count_4 + passenger_count_5 + passenger_count_6 + RatecodeID_1 + RatecodeID_2 + RatecodeID_3 + RatecodeID_4 + RatecodeID_5 + PULocation_Bronx + PULocation_Brooklyn + PULocation_EWR + PULocation_Manhattan + PULocation_Queens + PULocation_Staten_Island + DOLocation_Bronx + DOLocation_Brooklyn +  DOLocation_EWR + DOLocation_Manhattan + DOLocation_Queens +  DOLocation_Staten_Island + day_Fri + day_Mon + day_Sat + day_Sun + day_Thu + day_Tue + day_Wed + PU_time_of_day_Afternoon + PU_time_of_day_Evening + PU_time_of_day_Morning + PU_time_of_day_Night + DO_time_of_day_Afternoon + DO_time_of_day_Evening + DO_time_of_day_Morning + DO_time_of_day_Night, data=train, method='anova',control = c(cp = 0.001, maxdepth = 30))

rpart.plot(tip_amount_pt,extra = 100,type = 1,snip = TRUE)
```

```{r,echo=FALSE}
plotcp(tip_amount_pt)
```


```{r meterics1,echo=FALSE}

# Original
# Predict test values using tree
tree_amount_pred = predict(tip_amount_dt, newdata = test)

# Obtain MSE
tree_amount_mse = (sum((tree_amount_pred - test$tip_amount)^2)) / nrow(test)

# Print MSE
tree_perc_mse

# Obtain MAPE and r2
mape_ratio = MAPE(test$tip_amount, tree_amount_pred)
Rsquare_ratio = caret::R2(tree_amount_pred, test$tip_amount)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree",
  dependent = "tip_amount",
  mape = mape_ratio,
  Rsquare = Rsquare_ratio,
  AIC = '--'
))

# Prune
# Predict test values using tree
tree_amount_pred_prune = predict(tip_amount_pt, newdata = test)

# Obtain MSE
tree_amount_mse_prune = (sum((tree_amount_pred_prune - test$tip_amount)^2)) / nrow(test)

# Print MSE

tree_amount_mse_prune

# Obtain MAPE and r2
mape_amount_prune = MAPE(test$tip_amount, tree_amount_pred_prune)
Rsquare_amount_prune = caret::R2(tree_amount_pred_prune, test$tip_amount)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree (Prune)",
  dependent = "tip_amount",
  mape = mape_amount_prune,
  Rsquare = Rsquare_amount_prune,
  AIC = '--'
))

xkabledply(results_df, title = 'Grand Summary of All Models')

```

### 4.4.6 Random Forest

Finally, we apply our last model to further obtained enhanced results. The Random Forest builds on top of the classical decision tree by a method called **Bagging**

```{r Random_Forest1, results='markup', echo=FALSE}


rf.fit <- randomForest(tip_amount ~  trip_distance + fare_amount + tolls_amount + trip_duration + VendorID_1 
+ VendorID_2 + passenger_count_1 + passenger_count_2 + passenger_count_3 + passenger_count_4 + passenger_count_5 + passenger_count_6 + RatecodeID_1 + RatecodeID_2 + RatecodeID_3 + RatecodeID_4 + RatecodeID_5 + PULocation_Bronx + PULocation_Brooklyn + PULocation_EWR + PULocation_Manhattan + PULocation_Queens + PULocation_Staten_Island + DOLocation_Bronx + DOLocation_Brooklyn +  DOLocation_EWR + DOLocation_Manhattan + DOLocation_Queens + DOLocation_Staten_Island + day_Fri + day_Mon + day_Sat + day_Sun + day_Thu + day_Tue + day_Wed + PU_time_of_day_Afternoon + PU_time_of_day_Evening + PU_time_of_day_Morning + PU_time_of_day_Night + DO_time_of_day_Afternoon + DO_time_of_day_Evening + DO_time_of_day_Morning + DO_time_of_day_Night, data = train, ntree=100,keep.forest=FALSE, importance=TRUE)

print(rf.fit)

```


```{r ploting1 results,echo=FALSE}
ImpData <- as.data.frame(importance(rf.fit))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```


As seen from the above plot, the trip duration, duration of the trip, and fare amount has the highest impact on the model if they were to be removed.

### 4.4.7 Summary and analysis

The unpruned decision tree is the optimal model when aiming for a low MAPE, low AIC, and high r-squared. It is crucial to remember that when comparing the different models, the MAPE number is typically the same, hovering around 0.18 - 0.20, which denotes a 80-82 % accuracy. But all of the models' r-squared values are quite low, explaining about 7% to 15% of the variance in our dependent variable. As a result, this suggests that the models are neither thorough nor accurate fits.

## 4.5 Model Evalution Summary

```{r,echo=FALSE,results='markup'}

ezids::xkabledply(results_df, title = 'Summay for All Model\'s Evaluation Metrics',bso = 'hover')

```

# 5 Conclusion

##### We conclude our analysis by discussing the limitations and Future scopes of this project

#### Limitations

1. Daylight savings are not considered
2. In this will not consider large dataset (hardware )
3. In this there is no cash Tips are considered
4. There are more variables to consider such as Gender and Weather
5. human error in data
6. Central Limit Theorem (CLT) states that sample means of moderately large samples are often well-approximated by a normal distribution even if the data is not normally distributed. Our dataset contains a significant amount of observations thus qualifying it to be approximately normal under CLT. 

#### Future scope 

##### The dataset produced for this project will serve as the foundation for future study. More insights will be obtained by analyzing at least a year's worth of data. If weather impacts the amount of rides, hourly weather data combined with weather events may provide further information. A forecast and prediction based on zones, as well as boroughs, will make it extremely easy for drivers to be present at any particular moment in time and Gender and Driver and Trip Rating.
