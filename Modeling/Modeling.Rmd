---
title: "Predicting Gratuity Amount for Taxi Drivers"
author: "Chirag Lakhanpal, Shikha Sharma, Abhishek Pradhan"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

<style type="text/css">
  body{
  font-size: 10pt;
  font-family: 'Calibri';

@media (prefers-color-scheme: dark) {
  img {
    opacity: .75;
    transition: opacity .5s ease-in-out;
  }
  img:hover {
    opacity: 1;
  }
  }
}
</style>

```{r init, include=FALSE}
#remove.packages("vctrs")

#install.packages('MLmetrics')

# Installing necessary packages 
library(arrow)
library(ezids)
library(knitr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(reshape2)
library(caret)
library(fastDummies)
library(dataPreparation)
library(factoextra)
library(AICcmodavg)
library(modelr)
library(glmnet)
library(rpart)
library(MLmetrics)

# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
#options(scientific=T, digits = 3) 
options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
#options(scipen=999)
```

```{r Reading Data, results='markup',echo=FALSE}

# Loading main data for New York yellow taxi trips
dataset <- data.frame(read.csv('../Data/Yellow_Taxi_Trips_Data_Raw_Cleaned.csv'))
```

```{r difining data types, echo=FALSE}

# Converting Variables into Categorical Variables

colnames(dataset)

dataset$VendorID <- as.factor(dataset$VendorID)
dataset$passenger_count <- as.factor(dataset$passenger_count)
dataset$RatecodeID <- as.factor(dataset$RatecodeID)
dataset$PULocation <- as.factor(dataset$PULocation)
dataset$DOLocation <- as.factor(dataset$DOLocation)
dataset$day <- as.factor(dataset$day)
dataset$PU_time_of_day <- as.factor(dataset$PU_time_of_day)
dataset$DO_time_of_day <- as.factor(dataset$DO_time_of_day)

# Removing Payment type as we don't need it as all the tips considered are through credit card

dataset <- within(dataset, rm(payment_type))

```


# 4 Model Building

We began by selecting linear (univariate and multivariate) regression models to examine how they fit our data. Linear regression is a conventional, common approach that may explain the association with tip well, so we chose to test it first. To strengthen our linear model, we also used step-by-step feature selection, lasso, ridge, and principal component analysis (PCA). We also made use of decision trees. The capacity of decision trees to mimic non-linear connections is one of its advantages. According to our EDA, journey duration, distance, and fare are all linearly connected over small distances, but this connection weakens over longer distances due to the involvement of other possible factors.Consequently, there may be in fact a non-linear relationship with tip, too.

## 4.1 Preparation

We prepped our data for modeling before developing our models by using one hot encoding, establishing training and testing sets, and scaling our data.

### 4.1.1 One hot encoding 

We employed one hot encoding to convert factor columns to numerical columns. All factors will be converted into a distinct boolean column by a single hot encoding.

```{r OHE, echo=FALSE}

colnames(dataset)

# Extracting categorical vairables

fact <- c("VendorID","passenger_count","RatecodeID","PULocation", "DOLocation","day","PU_time_of_day","DO_time_of_day")

dataset <- dummy_cols(dataset, select_columns = fact)

# Removing the orignal vairable

dataset <- dataset[,!names(dataset) %in% fact]

```


Post One Hot Encoding (OHE) we are now left with **`r dim(dataset)[2]` columns**. 

### 4.1.2 Scaling variables

Because the magnitude of the values may not be proportionate, we must scale the numerical variables in our datasets. For comparative reasons, we compute the mean and standard deviation of each numerical column.

```{r Scaling,results='markup',echo=FALSE}
nums <- c('trip_distance','fare_amount','tip_amount','tolls_amount')

# Mean and SD of numerical columns

build_scales(data_set = dataset, cols = nums, verbose = TRUE)

dataset <- dataset %>%           # Applying functions of dplyr
  mutate_at(nums, ~(scale(.) %>% as.vector))
```

### 4.1.3 Test train split

In order to eliminate any bias in test results while utilizing train data, the train-test split should be implemented before (most) data modeling. We randomly divided the dataset into 70% train and 30% test to replicate a train and test set.

```{r Spliting, echo=FALSE}
set.seed(1)

#train and test split (Since we have enough data with can split our data into train (80%) & test (20%))
sample <- sample(c(TRUE, FALSE), nrow(dataset), replace=TRUE, prob=c(0.7,0.3))
train  <- dataset[sample, ]
test   <- dataset[!sample, ]


```
Number of rows of observations in training dataset is **`r nrow(train)`** and in testing dataset are **`r nrow(test)`** post split.

## 4.2 Evaluation Metrics

*From Wikipedia*

1. **Mean Absolute Percentage Error (MAPE)**

|               The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction 
|               accuracy of a forecasting method in statistics. It usually expresses the accuracy as a ratio defined by the formula:

|               \n![](Mape.svg)

2. **Akaike information criterion (AIC)**

|               The Akaike information criterion is an estimator of prediction error and thereby relative quality of statistical models for a 
|               given set of data.Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the  
|               other models.:

|               \n![](AIC.svg)

3. **R squared (R^2^)**

|               In statistics, the coefficient of determination, denoted R2 or r2 and pronounced "R squared", is the proportion of the variation 
|               in the dependent variable that is predictable from the independent variable(s).

|               \n![](R-squared.svg)

## 4.3 Tip Percentage

The subsequent step post preparing our data is to employ a number of regression-based methods to extract insights from data, which we can then use to predict which result is likely to hold true for our target variable based on training data.

### 4.3.1 Principal component analysis

We chose PCA as a variable reduction strategy because the majority of our variables were associated with one another and there were 48 features.

```{r pca,echo=FALSE}

# get principal components using prcomp, we have used scale = F as the data is already scaled
pr.out =prcomp(train %>% select_if(is.numeric), scale = F)
summary(pr.out)

fviz_eig(pr.out, addlabels = TRUE)

```
As 

```{r,echo=FALSE}
fviz_pca_var(pr.out,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

Variables graph. Variables that are positively associated point to the same side of the plot. Negatively associated variables point to the graph's opposing sides.

**Observations:** Even if the components explain 95% of the variance in the data, that may not necessarily mean that a good R2 or high coefficients will result. Hence to investigate this, we evaluated this; we proceed to build out Linear regression mode.

### 4.3.2 Linear regression

From the results of the Principle component analysis, we constructed a linear model with tip percentage along with the first three high variability explainers and correlated values for 'Tip_Amount': fare_amount (-0.29), trip_distance (-0.28), and trip_duration (-0.17).

```{r Linear Models,echo=FALSE}
#Taking best three variables correlated with tip_fare ratio based on Pearson Correlation

#linear model of tip_fare_ratio ~ trip_duration
fit1 <- lm(tip_perc ~ trip_duration, data = train )
#summary(fit1)
# vif(fit1)
# plot(fit1)


#linear model of tip_fare_ratio ~ trip_duration+fare_amount
fit2 <- lm(tip_perc ~ trip_duration+fare_amount, data = train )
#summary(fit2)
# vif(fit2)
# plot(fit2)


#linear model of tip_fare_ratio ~ trip_duration+fare_amount+trip_distance
fit3 <- lm(tip_perc ~ trip_duration+fare_amount+trip_distance, data = train )
#summary(fit3)
#vif(fit3)
#plot(fit3)
```

```{r AIC of Linear Models, echo=FALSE}
#define list of models
models <- list(fit1, fit2, fit3)

#specify model names
mod.names <- c('trip_duration.trip_duration.fare_amount.trip_distance', 'trip_duration.trip_duration.fare_amount', 'trip_duration.trip_duration')

#calculate AIC of each model
AIC <- aictab(cand.set = models, modnames = mod.names)
```

**ANOVA tests on all the three models**

The summary for three linear models is:

 Fit | Model Equation | R^2 | ANOVA P-value | AIC |  
 ----|----------------------|-----|-----------|-----|  
 1   | tip_perc ~ trip_duration | `r format(summary(fit1)$r.squared, digits=3)` | -- | `r AIC[,3][1]` |
 2   | tip_perc ~ trip_duration+fare_amount |`r format(summary(fit2)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][2])` | `r AIC[,3][2]` |
 3   | tip_perc ~ trip_duration+fare_amount+trip_distance |`r format(summary(fit3)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][3], digits=3)` | `r AIC[,3][3]` |


**Observations:** Looking at the combination of p-value and r-squared, we conclude that fit3 performs slightly better than the other two fits. Hence, we check if there is an improvement in model 3 in the absence of outliers.

```{r, include=TRUE, echo=FALSE}
#Prediction

model.final.pred <- add_predictions(test,fit3)

# Model performance metrics

results_df <- data.frame(
  technique = 'Linear(3 vars with best cor-coeffs)',
  dependent = 'tip_perc',
  mape = MAPE(model.final.pred$tip_perc, model.final.pred$pred),
  Rsquare = caret::R2(model.final.pred$pred, model.final.pred$tip_perc)
)

```

**Treating Outliers and Modeling**

```{r Removing Outliers, echo=FALSE}

train_no <- train

outlierKD2(train_no,var = trip_duration,boxplt = T,rm = T)
outlierKD2(train_no,var = fare_amount,boxplt = T,rm = T)
outlierKD2(train_no,var = trip_distance,boxplt = T, rm = T)

model_no <- lm(tip_perc ~ trip_duration+fare_amount+trip_distance, data = train_no)

summary(model_no)

predictions <- predict(model_no, test)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear-treated outlier",
  dependent = "tip_perc",
  mape = MAPE(test$tip_perc, predictions),
  Rsquare = caret::R2(predictions, test$tip_perc)
))
results_df
```

Even after treating the outliers in our model three fit, There is little to no difference in the results, and the r-squared and MAPE values remains `r format(results_df[2, 4], digits=3)` and `r format(results_df[2, 3], digits=4)`.

### 4.3.3 Lasso regression

Lasso regression is a form of regularization (L1) approach that might result in coefficients that are canceled out (in other words, some of the features are completely neglected for the evaluation of output). As a result, it not only helps to reduce over-fitting, but it may also aid in feature selection.

As we increase the value of lambda, the bias increase and  variance decrease, so we Iterated through a set of lambda values to find the optimum value. The graph below shows how lasso reduces the value of unnecessary attribute coefficients to 0. Only the five attributes with the greatest coefficient values are indicated for greater visibility.

**Note:** Unfortunately, due to hardware limitations, the dataset is reduced to 15,000 records for this regression technique.

```{r lasso_tipfareratio1, include=FALSE, echo=FALSE}

reduced_train <- sample_n(train, 15000)

# Prep model parameters
x=model.matrix(tip_perc~.-tip_amount,reduced_train)[,-1]

tail(x)

y = reduced_train %>%
  select(tip_perc) %>%
  unlist() %>%
  as.numeric()
  
# Find the best lambda using 10 cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1, standardize = FALSE)
# Display the best lambda value
lasso_cv$lambda.min

# Plot lambda CV
plot(cv_lamda)

# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min, standardize = FALSE)
# Dsiplay regression coefficients
coef(lasso_model)

# # Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Lasso",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))

# Plot predicted vs actual values
ggplot(test, aes(x =test$tip_fare_ratio ,y = predictions)) +
  geom_point() +
  labs(title="Predicted vs Actual Values",
       x="Fare amount ", y = "Tip amount")

```




