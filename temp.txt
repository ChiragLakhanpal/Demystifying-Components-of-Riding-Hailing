Structure 
Model Structure
ONE HOT ENCODING 
Scalling
Test and Train
Evlution Mettrics
Mean Aboslute Perntge Error 
AIC
R square
From Wikipedia*

1. **Mean Absolute Percentage Error (MAPE)**

|               The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction 
|               accuracy of a forecasting method in statistics. It usually expresses the accuracy as a ratio defined by the formula:

|               \n![](Mape.svg)

2. **Akaike information criterion (AIC)**

|               The Akaike information criterion is an estimator of prediction error and thereby relative quality of statistical models for a 
|               given set of data.Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the  
|               other models.:

|               \n![](AIC.svg)

3. **R squared (R^2^)**

|               In statistics, the coefficient of determination, denoted R2 or r2 and pronounced "R squared", is the proportion of the variation 
|               in the dependent variable that is predictable from the independent variable(s).

|               \n![](R-squared.svg)
From Wikipedia*

1. **Mean Absolute Percentage Error (MAPE)**

|               The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction 
|               accuracy of a forecasting method in statistics. It usually expresses the accuracy as a ratio defined by the formula:

|               \n![](Mape.svg)

2. **Akaike information criterion (AIC)**

|               The Akaike information criterion is an estimator of prediction error and thereby relative quality of statistical models for a 
|               given set of data.Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the  
|               other models.:

|               \n![](AIC.svg)

3. **R squared (R^2^)**

|               In statistics, the coefficient of determination, denoted R2 or r2 and pronounced "R squared", is the proportion of the variation 
|               in the dependent variable that is predictable from the independent variable(s).

|               \n![](R-squared.svg)
Lasso regression is a form of regularization (L1) approach that might result in coefficients that are canceled out (in other words, some of the features are completely neglected for the evaluation of output). As a result, it not only helps to reduce over-fitting, but it may also aid in feature selection.

As we increase the value of lambda, the bias increase and  variance decrease, so we Iterated through a set of lambda values to find the optimum value. The graph below shows how lasso reduces the value of unnecessary attribute coefficients to 0. Only the five attributes with the greatest coefficient values are indicated for greater visibility.

he coefficients that best suit the data are discovered using the least squares approach. It should also determine the unbiased coefficients as a further requirement. Here, unbiased refers to the fact that OLS ignores the independent variables' relative importance. A given data set's coefficients are easily found. In other words, the lowest "Residual Sum of Squares (RSS)" can only be obtained from one set of betas. It therefore poses a question whether the model with the lowest RSS is actually the better model.

In a sense, OLS offers the model with the highest variance and the lowest bias, and it gets more complex as the number of variables rises. Although it is stationary and never moves, we still want a model with little bias and little variance. This void can be filled by Ridge, which is also known as regularization. Since the ridge regression penalizes coefficients, the least effective ones in the estimation will "shrink" the quickest. In ridge regression, the lambda parameter (penalizing factor) can be adjusted to alter the model coefficients.

The classification and regression tree (CART) methodology is one of the earliest methods for creating regression trees, however there are many more. A data set is divided into smaller subgroups by basic regression trees, which then fit a straightforward constant to each observation in each segment. By using successive binary partitions (also known as recursive partitioning) depending on several predictors, the partitioning is accomplished.

To enhance prediction performance on certain unknown data, a balance in the depth and complexity of the tree is generally required. To achieve this balance, we generally create a very big tree and then prune it back to identify an ideal subtree. We identify the best subtree by applying a cost complexity parameter (Î±) that penalizes our objective function for the number of terminal nodes in the tree.


Finally, we apply our last model to further obtained enhanced results. The Random Forest builds on top of the classical deccission tree by a method called **Bagging**
Node purity is the total decrease in residual sum of squares when splitting on a variable averaged over all trees (i.e. how well a predictor decreases variance). Importance gives you what the model has learnt. The above plot shows, for each variable, how important it is in classifying the data. The Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance.


As we increase the value of lambda, the bias increase and  variance decrease, so we Iterated through a set of lambda values to find the optimum value. The graph below shows how lasso reduces the value of unnecessary attribute coefficients to 0. Only the five attributes with the greatest coefficient values are indicated for greater visibility.

The unpruned decision tree is the optimal model when aiming for a low MAPE, low AIC, and high r-squared. It is crucial to remember that when comparing the different models, the MAPE number is typically the same, hovering around 0.18 - 0.20, which denotes a 80-82 % accuracy. But all of the models' r-squared values are quite low, explaining about 7% to 15% of the variance in our dependent variable. As a result, this suggests that the models are neither thorough nor accurate fits.

## 4.4 Tip Amount
The subsequent step post preparing our data is to employ a number of regression-based methods to extract insights from data, which we can then use to predict which result is likely to hold true for our target variable based on training data.

### 4.4.1 Principal component analysis

We chose PCA as a variable reduction strategy because the majority of our variables were associated with one another and there were 48 features.

# 4 Model Building

We began by selecting linear (univariate and multivariate) regression models to examine how they fit our data. Linear regression is a conventional, common approach that may explain the association with tip well, so we chose to test it first. To strengthen our linear model, we also used step-by-step feature selection, lasso, ridge, and principal component analysis (PCA). We also made use of decision trees. The capacity of decision trees to mimic non-linear connections is one of its advantages. According to our EDA, journey duration, distance, and fare are all linearly connected over small distances, but this connection weakens over longer distances due to the involvement of other possible factors.Consequently, there may be in fact a non-linear relationship with tip, too.

## 4.1 Preparation

We prepped our data for modeling before developing our models by using one hot encoding, establishing training and testing sets, and scaling our data.

### 4.1.1 One hot encoding 

We employed one hot encoding to convert factor columns to numerical columns. All factors will be converted into a distinct boolean column by a single hot encoding.

Post One Hot Encoding (OHE) we are now left with **`r dim(dataset)[2]` columns**. 

### 4.1.2 Scaling variables

Because the magnitude of the values may not be proportionate, we must scale the numerical variables in our datasets. For comparative reasons, we compute the mean and standard deviation of each numerical column.
### 4.1.3 Test train split

In order to eliminate any bias in test results while utilizing train data, the train-test split should be implemented before (most) data modeling. We randomly divided the dataset into 70% train and 30% test to replicate a train and test set.
## 4.2 Evaluation Metrics

*From Wikipedia*

1. **Mean Absolute Percentage Error (MAPE)**

|               The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction 
|               accuracy of a forecasting method in statistics. It usually expresses the accuracy as a ratio defined by the formula:

|               \n![](Mape.svg)

2. **Akaike information criterion (AIC)**

|               The Akaike information criterion is an estimator of prediction error and thereby relative quality of statistical models for a 
|               given set of data.Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the  
|               other models.:

|               \n![](AIC.svg)

3. **R squared (R^2^)**

|               In statistics, the coefficient of determination, denoted R2 or r2 and pronounced "R squared", is the proportion of the variation 
|               in the dependent variable that is predictable from the independent variable(s).

|               \n![](R-squared.svg)
## 4.3 Tip Percentage
The subsequent step post preparing our data is to employ a number of regression-based methods to extract insights from data, which we can then use to predict which result is likely to hold true for our target variable based on training data.
### 4.3.1 Principal component analysis
We chose PCA as a variable reduction strategy because the majority of our variables were associated with one another and there were 48 features.

Variables graph. Variables that are positively associated point to the same side of the plot. Negatively associated variables point to the graph's opposing sides.

**Observations:** Even if the components explain 93.4% of the variance in the data, that may not necessarily mean that a good R^2^ or high coefficients will result. Hence to investigate this, we evaluated this; we proceed to build out Linear regression mode.

### 4.3.2 Linear regression
From the results of the Principle component analysis, we constructed a linear model with tip percentage along with the first three high variability explainers and correlated values for 'Tip_Amount': fare_amount (-0.29), trip_distance (-0.28), and trip_duration (-0.17).


**ANOVA tests on all the three models**
The summary for three linear models is:

 Fit | Model Equation | R^2 | ANOVA P-value | AIC |  
 ----|----------------------|-----|-----------|-----|  
 1   | tip_perc ~ trip_duration | `r format(summary(fit1)$r.squared, digits=3)` | -- | `r AIC[,3][1]` |
 2   | tip_perc ~ trip_duration+fare_amount |`r format(summary(fit2)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][2])` | `r AIC[,3][2]` |
 3   | tip_perc ~ trip_duration+fare_amount+trip_distance |`r format(summary(fit3)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][3], digits=3)` | `r AIC[,3][3]` |


**Observations:** Looking at the combination of p-value and r-squared, we conclude that fit3 performs slightly better than the other two fits. Hence, we check if there is an improvement in model 3 in the absence of outliers.
**Treating Outliers and Modeling**
### 4.3.3 Lasso regression
Lasso regression is a form of regularization (L1) approach that might result in coefficients that are canceled out (in other words, some of the features are completely neglected for the evaluation of output). As a result, it not only helps to reduce over-fitting, but it may also aid in feature selection.

As we increase the value of lambda, the bias increase and  variance decrease, so we Iterated through a set of lambda values to find the optimum value. The graph below shows how lasso reduces the value of unnecessary attribute coefficients to 0. Only the five attributes with the greatest coefficient values are indicated for greater visibility.
### 4.3.4 Ridge regression
he coefficients that best suit the data are discovered using the least squares approach. It should also determine the unbiased coefficients as a further requirement. Here, unbiased refers to the fact that OLS ignores the independent variables' relative importance. A given data set's coefficients are easily found. In other words, the lowest "Residual Sum of Squares (RSS)" can only be obtained from one set of betas. It therefore poses a question whether the model with the lowest RSS is actually the better model.

In a sense, OLS offers the model with the highest variance and the lowest bias, and it gets more complex as the number of variables rises. Although it is stationary and never moves, we still want a model with little bias and little variance. This void can be filled by Ridge, which is also known as regularization. Since the ridge regression penalizes coefficients, the least effective ones in the estimation will "shrink" the quickest. In ridge regression, the lambda parameter (penalizing factor) can be adjusted to alter the model coefficients.

Again, Only the five attributes with the greatest coefficient values are indicated for greater visibility.
### 4.3.5 Decision tree
The classification and regression tree (CART) methodology is one of the earliest methods for creating regression trees, however there are many more. A data set is divided into smaller subgroups by basic regression trees, which then fit a straightforward constant to each observation in each segment. By using successive binary partitions (also known as recursive partitioning) depending on several predictors, the partitioning is accomplished.

**Cost complexity criterion**

To enhance prediction performance on certain unknown data, a balance in the depth and complexity of the tree is generally required. To achieve this balance, we generally create a very big tree and then prune it back to identify an ideal subtree. We identify the best subtree by applying a cost complexity parameter (Î±) that penalizes our objective function for the number of terminal nodes in the tree.

##### Pruned Decision Tree
Pruning the decision tree to 16 variables gives a much better model as seen below.
### 4.3.6 Random Forest
Finally, we apply our last model to further obtained enhanced results. The Random Forest builds on top of the classical deccission tree by a method called **Bagging**
Node purity is the total decrease in residual sum of squares when splitting on a variable averaged over all trees (i.e. how well a predictor decreases variance). Importance gives you what the model has learnt. The above plot shows, for each variable, how important it is in classifying the data. The Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance.
### 4.3.7 Summary and analysis

The unpruned decision tree is the optimal model when aiming for a low MAPE, low AIC, and high r-squared. It is crucial to remember that when comparing the different models, the MAPE number is typically the same, hovering around 0.18 - 0.20, which denotes a 80-82 % accuracy. But all of the models' r-squared values are quite low, explaining about 7% to 15% of the variance in our dependent variable. As a result, this suggests that the models are neither thorough nor accurate fits.

## 4.4 Tip Amount
The subsequent step post preparing our data is to employ a number of regression-based methods to extract insights from data, which we can then use to predict which result is likely to hold true for our target variable based on training data.

### 4.4.1 Principal component analysis

We chose PCA as a variable reduction strategy because the majority of our variables were associated with one another and there were 48 features.
Variables graph. Variables that are positively associated point to the same side of the plot. Negatively associated variables point to the graph's opposing sides.

**Observations:** Even if the components explain 94% of the variance in the data, that may not necessarily mean that a good R^2^ or high coefficients will result. Hence to investigate this, we evaluated this; we proceed to build out Linear regression mode.


### 4.4.2 Linear regression

From the results of the Principle component analysis, we constructed a linear model with tip amount along with the first three high variability explainers and correlated values for 'Tip_Amount': fare_amount (-0.29), trip_distance (-0.28), and trip_duration (-0.17).

**ANOVA tests on all the three models**

The summary for three linear models is:

 Fit | Model Equation | R^2 | ANOVA P-value | AIC |  
 ----|----------------------|-----|-----------|-----|  
 1   | tip_amount ~ trip_duration | `r format(summary(fit1)$r.squared, digits=3)` | -- | `r AIC[,3][1]` |
 2   | tip_amount ~ trip_duration+fare_amount |`r format(summary(fit2)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][2])` | `r AIC[,3][2]` |
 3   | tip_amount ~ trip_duration+fare_amount+trip_distance |`r format(summary(fit3)$r.squared, digits=3)` | `r format(anova(fit1,fit2,fit3)[6][["Pr(>F)"]][3], digits=3)` | `r AIC[,3][3]` |


**Observations:** Looking at the combination of p-value and r-squared, we conclude that fit3 performs slightly better than the other two fits. Hence, we check if there is an improvement in model 3 in the absence of outliers.

**Treating Outliers and Modeling**

Even after treating the outliers in our model three fit, There is little to no difference in the results, and the r-squared and MAPE values remains `r format(results_df[2, 4], digits=3)` and `r format(results_df[2, 3], digits=4)`.

### 4.4.3 Lasso regression

Lasso regression is a form of regularization (L1) approach that might result in coefficients that are canceled out (in other words, some of the features are completely neglected for the evaluation of output). As a result, it not only helps to reduce over-fitting, but it may also aid in feature selection.

As we increase the value of lambda, the bias increase and  variance decrease, so we Iterated through a set of lambda values to find the optimum value. The graph below shows how lasso reduces the value of unnecessary attribute coefficients to 0. Only the five attributes with the greatest coefficient values are indicated for greater visibility.

The lambda value that minimizes the test MSE turns out to be **`r lasso_cv$lambda.min`** .

As with before, the r-squared is only 11%, which means there is room for a lot improvement.


### 4.4.4 Ridge regression

The coefficients that best suit the data are discovered using the least squares approach. It should also determine the unbiased coefficients as a further requirement. Here, unbiased refers to the fact that OLS ignores the independent variables' relative importance. A given data set's coefficients are easily found. In other words, the lowest "Residual Sum of Squares (RSS)" can only be obtained from one set of betas. It therefore poses a question whether the model with the lowest RSS is actually the better model.

In a sense, OLS offers the model with the highest variance and the lowest bias, and it gets more complex as the number of variables rises. Although it is stationary and never moves, we still want a model with little bias and little variance. This void can be filled by Ridge, which is also known as regularization. Since the ridge regression penalizes coefficients, the least effective ones in the estimation will "shrink" the quickest. In ridge regression, the lambda parameter (penalizing factor) can be adjusted to alter the model coefficients.

Again, Only the five attributes with the greatest coefficient values are indicated for greater visibility.

The lambda value that minimizes the test MSE turns out to be **`r ridge_cv$lambda.min`** .
The Plot shows that all the vairables explain ~11% (~0.108 point on the plot) of the variance in the data. Same is bolstered by the R^2^ value of the model.

### 4.4.5 Decision tree

The classification and regression tree (CART) methodology is one of the earliest methods for creating regression trees, however there are many more. A data set is divided into smaller subgroups by basic regression trees, which then fit a straightforward constant to each observation in each segment. By using successive binary partitions (also known as recursive partitioning) depending on several predictors, the partitioning is accomplished.

**Cost complexity criterion**

To enhance prediction performance on certain unknown data, a balance in the depth and complexity of the tree is generally required. To achieve this balance, we generally create a very big tree and then prune it back to identify an ideal subtree. We identify the best subtree by applying a cost complexity parameter (Î±) that penalizes our objective function for the number of terminal nodes in the tree.
When we consider all the variables while building our decision tree, the model quickly becomes overfitted.
The above shows the compares the error over the range of Î±'s (cost complexity - cp value at the bottom X-axis). The upper X-axis gives the number of nodes. We can see returns deminish after around 16 leafs (dashed vertical line.


##### Pruned Decision Tree

Pruning the decision tree to 16 variables gives a much better model as seen below

### 4.4.6 Random Forest

Finally, we apply our last model to further obtained enhanced results. The Random Forest builds on top of the classical deccission tree by a method called **Bagging**
### 4.3.7 Summary and analysis

The unpruned decision tree is the optimal model when aiming for a low MAPE, low AIC, and high r-squared. It is crucial to remember that when comparing the different models, the MAPE number is typically the same, hovering around 0.18 - 0.20, which denotes a 80-82 % accuracy. But all of the models' r-squared values are quite low, explaining about 7% to 15% of the variance in our dependent variable. As a result, this suggests that the models are neither thorough nor accurate fits.
